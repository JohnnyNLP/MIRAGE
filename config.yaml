mode: "RAG"
LLM_info:
  LLM_repo: ["meta-llama/Llama-2-7b-chat-hf", "Qwen/Qwen2-7B-Instruct", "gpt-3.5-turbo", "gpt-4o"]
  prompt_path: "prompt.yaml"
  vllm_configs:
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.9
    dtype: "float16"
    sampling_params:
      temperature: 0.1
      top_p: 0.1
      max_tokens: 256
  preset_prompt: true
  save_directory: "Inference_result"
retriever_info:
  retriever_repo: ["BAAI/bge-small-en-v1.5", "BAAI/bge-base-en-v1.5", "BAAI/bge-large-en-v1.5", "facebook/contriever", "nvidia/NV-Embed-v2"]
  top_k: 5
  retriever_batch_size: 1024
  use_cuda: true
  save_directory: "Retrieval_result"
prompt:
  system_prompt: |
    You are a helpful assistant.
  base_format: |
    Question: {query}
    
    Answer : 
  RAG_format: |
    Question : {query}
    
    Context : {context}
    
    Answer :
